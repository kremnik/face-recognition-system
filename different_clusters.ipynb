{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import math\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import facenet\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, estimate_bandwidth, Birch\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_filenames(model_dir):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files)==0:\n",
    "        raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    elif len(meta_files)>1:\n",
    "        raise ValueError('There should not be more than one meta file in the model directory (%s)' % model_dir)\n",
    "    meta_file = meta_files[0]\n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups())>=2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return meta_file, ckpt_file\n",
    "\n",
    "def load_model(model, sess):\n",
    "    model_exp = os.path.expanduser(model)\n",
    "    print('Model directory: %s' % model_exp)\n",
    "    meta_file, ckpt_file = get_model_filenames(model_exp)\n",
    "\n",
    "    print('Metagraph file: %s' % meta_file)\n",
    "    print('Checkpoint file: %s' % ckpt_file)\n",
    "\n",
    "    saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file))\n",
    "    saver.restore(sess, os.path.join(model_exp, ckpt_file))\n",
    "    \n",
    "def forward(paths, batch_size):\n",
    "    nrof_images = len(paths)\n",
    "    nrof_batches = int(math.ceil(1.0*nrof_images / batch_size))\n",
    "    emb_array = np.zeros((nrof_images, embedding_size))\n",
    "\n",
    "    for i in range(nrof_batches):    \n",
    "        start_index = i*batch_size\n",
    "        end_index = min((i+1)*batch_size, nrof_images)\n",
    "        paths_batch = paths[start_index:end_index]\n",
    "        images = facenet.load_data(paths_batch, False, False, image_size)\n",
    "        feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n",
    "        emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)    \n",
    "        print(\"batch number:\", i)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    return emb_array\n",
    "\n",
    "def printDistance(emb_array, label_list, end=10):\n",
    "    threshold = 1.0\n",
    "    lng = len(emb_array[:end])\n",
    "    print('Distance matrix')\n",
    "    print('    ', end='')\n",
    "    for i in range(lng):\n",
    "        print('    %3d     ' % i, end='')\n",
    "    print('')\n",
    "    for i in range(lng):\n",
    "        print('%1d  ' % i, end='')\n",
    "        for j in range(lng):\n",
    "            dist = np.linalg.norm(emb_array[i,:] - emb_array[j,:])\n",
    "            print('  %1.4f' % dist, end='')\n",
    "            if (label_list[i] == label_list[j]) and (dist <= threshold):\n",
    "                print(\"(TP)\", end='')\n",
    "            elif (label_list[i] != label_list[j]) and (dist > threshold):\n",
    "                print(\"(TN)\", end='')\n",
    "            elif (label_list[i] == label_list[j]) and (dist > threshold):\n",
    "                print(\"\\033[93m(FN)\\033[0m\", end='')\n",
    "            elif (label_list[i] != label_list[j]) and (dist <= threshold):\n",
    "                print(\"\\033[93m(FP)\\033[0m\", end='')\n",
    "        print('')\n",
    "        \n",
    "def printData(labels, paths, number):\n",
    "    for i in range(number):\n",
    "        if i > 0 and labels[i] != labels[i-1]:\n",
    "            print('')\n",
    "        print('{0:3d} {1:3d}   {2:s}'.format(i, labels[i], paths[i].split(\"\\\\\")[1]))\n",
    "        # print('{0:3d}   {1:s}'.format(labels[i], paths[i].split(\"\\\\\")[1]))\n",
    "        \n",
    "def PCAtransform(emb_array, n_components = 10):\n",
    "    pca = PCA(n_components, whiten=True)\n",
    "    pca.fit(emb_array)\n",
    "    return pca.transform(emb_array), pca\n",
    "\n",
    "def splitTrainTest(data_set, percent_train):\n",
    "    paths, labels_list = facenet.get_image_paths_and_labels(data_set)\n",
    "    \n",
    "    lng = len(paths)\n",
    "    lng_train = int(lng * percent_train / 100);\n",
    "    lng_test = lng - lng_train\n",
    "    \n",
    "    train_paths = paths[0:lng_train]\n",
    "    test_paths = paths[lng_train:]\n",
    "    \n",
    "    train_labels = labels_list[0:lng_train]\n",
    "    test_labels = labels_list[lng_train:]\n",
    "    \n",
    "    return train_paths, train_labels, test_paths, test_labels\n",
    "\n",
    "def preprocessing(X, num_components = 50):\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    X = X - np.mean(X, axis=0) # centering\n",
    "    X = X / X.std(axis=0) # standardization\n",
    "\n",
    "    # PCA\n",
    "    cov = np.cov(X.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "\n",
    "    for ev in eig_vecs:\n",
    "        np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "\n",
    "    # Make a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "    # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "    eig_pairs.sort()\n",
    "    eig_pairs.reverse()\n",
    "\n",
    "    pca_matrix = eig_pairs[0][1].reshape(num_features, 1)\n",
    "\n",
    "    for i in range(1, num_components):\n",
    "        pca_matrix = np.hstack((pca_matrix,\n",
    "                                eig_pairs[i][1].reshape(num_features, 1)))\n",
    "\n",
    "    X = X.dot(pca_matrix)\n",
    "\n",
    "    # Whitening\n",
    "    X = X / np.sqrt(eig_vals[:num_components])\n",
    "    \n",
    "    return X\n",
    "    \n",
    "\n",
    "def trainFisher(emb_array, y):\n",
    "    emb_del = np.empty_like(emb_array)\n",
    "    np.copyto(emb_del, emb_array)\n",
    "    \n",
    "    emb_array = np.vstack((emb_array, y))\n",
    "    \n",
    "    E = np.linalg.inv(np.cov(emb_array.T))\n",
    "    D = (y - (1 / (len(emb_array) - 1) * np.sum(emb_del)))\n",
    "    \n",
    "    w = np.dot(E, D)\n",
    "    c = np.dot(w / np.linalg.norm(w), y)\n",
    "    \n",
    "    return c, w\n",
    "\n",
    "def trainFisherWithNum(emb_array, sample_number):    \n",
    "    y = emb_array[sample_number]\n",
    "    emb_del = np.delete(emb_array, (sample_number), axis=0)\n",
    "    \n",
    "    E = np.linalg.inv(np.cov(emb_array.T))\n",
    "    D = (y - (1 / (len(emb_array) - 1) * np.sum(emb_del)))\n",
    "    \n",
    "    w = np.dot(E, D)\n",
    "    c = np.dot(w / np.linalg.norm(w), y)\n",
    "    \n",
    "    return c, w   \n",
    "\n",
    "def Fisher(w, c, sample):\n",
    "    return np.dot(w / np.linalg.norm(w), sample) - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: 20170511-185253\n",
      "Metagraph file: model-20170511-185253.meta\n",
      "Checkpoint file: model-20170511-185253.ckpt-80000\n",
      "WARNING:tensorflow:The saved meta_graph is possibly from an older release:\n",
      "'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.\n",
      "INFO:tensorflow:Restoring parameters from 20170511-185253\\model-20170511-185253.ckpt-80000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "data_set = facenet.get_dataset(\"../../datasets/lfw/lfw_mtcnnpy_160/\")\n",
    "train_paths, train_labels, test_paths, test_labels = splitTrainTest(data_set, 100)\n",
    "load_model(\"20170511-185253\", sess)\n",
    "\n",
    "images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "image_size = 160\n",
    "embedding_size = embeddings.get_shape()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_array = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything ok!\n"
     ]
    }
   ],
   "source": [
    "t = emb_array\n",
    "\n",
    "# Centering\n",
    "tc = t - np.mean(t, axis=0)\n",
    "\n",
    "# Standartization\n",
    "ts = tc / tc.std(axis=0)\n",
    "\n",
    "# PCA\n",
    "X = ts\n",
    "\n",
    "num_components = 50\n",
    "\n",
    "cov = np.cov(X.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "\n",
    "for ev in eig_vecs:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "print('Everything is ok!')\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "pca_matrix = eig_pairs[0][1].reshape(128, 1)\n",
    "\n",
    "for i in range(1, num_components):\n",
    "    pca_matrix = np.hstack((pca_matrix,\n",
    "                            eig_pairs[i][1].reshape(128, 1)))\n",
    "\n",
    "X_pca = X.dot(pca_matrix)\n",
    "\n",
    "# Whitening\n",
    "X_whit = X_pca / np.sqrt(eig_vals[:num_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fisher's discriminant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  0.000616998303255\n",
      "Recall:     1.0\n"
     ]
    }
   ],
   "source": [
    "bush_label = 5\n",
    "idx, = np.where(train_labels == bush_label)\n",
    "\n",
    "X_work = np.delete(X_whit, idx, axis=0)\n",
    "c, w = trainFisher(X_work, X_whit[idx[0]])\n",
    "\n",
    "lng = len(X_whit)\n",
    "summ = 0\n",
    "thr = 0.0\n",
    "answers = []\n",
    "\n",
    "for i in range(lng):\n",
    "    l = Fisher(w, thr, X_whit[i]/np.linalg.norm(X_whit[i]))\n",
    "    answers.append(l)\n",
    "    \n",
    "answers = np.asarray(answers)\n",
    "true_labels = np.array(train_labels == bush_label, dtype=int)\n",
    "prediction = np.array(answers > 0, dtype=int)\n",
    "\n",
    "print(\"Precision: \", metrics.precision_score(true_labels, prediction))\n",
    "print(\"Recall:    \", metrics.recall_score(true_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FN and FP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements:              4\n",
      "False Negatives:       0\n",
      "False Negatives Rate:  0.0\n",
      "\n",
      "Elements:              13229\n",
      "False Positives:       6479\n",
      "False Positives Rate:  0.48975735127371683\n"
     ]
    }
   ],
   "source": [
    "fn = len(prediction[idx][prediction[idx] != true_labels[idx]])\n",
    "fnr = fn / len(true_labels[idx])\n",
    "\n",
    "print(\"Elements:             \", len(prediction[idx]))\n",
    "print(\"False Negatives:      \", fn)\n",
    "print(\"False Negatives Rate: \", fnr)\n",
    "print(\"\")\n",
    "\n",
    "idx2, = np.where(train_labels != bush_label)\n",
    "fp = len(prediction[idx2][prediction[idx2] != true_labels[idx2]])\n",
    "fpr = fp / len(true_labels[idx2])\n",
    "\n",
    "print(\"Elements:             \", len(prediction[idx2]))\n",
    "print(\"False Positives:      \", fp)\n",
    "print(\"False Positives Rate: \", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plots**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.,  1.]),\n",
       " array([ 0.58292861,  0.61800747,  0.65308632,  0.68816517,  0.72324403,\n",
       "         0.75832288,  0.79340174,  0.82848059,  0.86355945,  0.8986383 ,\n",
       "         0.93371715]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 530,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x55a80b9cf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(answers[4:100][true_labels[4:100] == 0], histtype=\"bar\")\n",
    "plt.hist(answers[4:100][true_labels[4:100] == 1], histtype=\"bar\", color='orange')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05724639,  0.93371715,  0.77620345,  0.79686176,  0.58292861,\n",
       "        0.03708661])"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.93371715,  0.77620345,  0.79686176,  0.58292861])"
      ]
     },
     "execution_count": 495,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[4:10][true_labels[4:10] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.05724639,  0.03708661])"
      ]
     },
     "execution_count": 496,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers[4:10][true_labels[4:10] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
