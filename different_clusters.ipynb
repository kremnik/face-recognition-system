{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import math\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import facenet\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, estimate_bandwidth, Birch\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_filenames(model_dir):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files)==0:\n",
    "        raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    elif len(meta_files)>1:\n",
    "        raise ValueError('There should not be more than one meta file in the model directory (%s)' % model_dir)\n",
    "    meta_file = meta_files[0]\n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups())>=2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return meta_file, ckpt_file\n",
    "\n",
    "def load_model(model, sess):\n",
    "    model_exp = os.path.expanduser(model)\n",
    "    print('Model directory: %s' % model_exp)\n",
    "    meta_file, ckpt_file = get_model_filenames(model_exp)\n",
    "\n",
    "    print('Metagraph file: %s' % meta_file)\n",
    "    print('Checkpoint file: %s' % ckpt_file)\n",
    "\n",
    "    saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file))\n",
    "    saver.restore(sess, os.path.join(model_exp, ckpt_file))\n",
    "    \n",
    "def forward(paths, batch_size):\n",
    "    nrof_images = len(paths)\n",
    "    nrof_batches = int(math.ceil(1.0*nrof_images / batch_size))\n",
    "    emb_array = np.zeros((nrof_images, embedding_size))\n",
    "\n",
    "    for i in range(nrof_batches):    \n",
    "        start_index = i*batch_size\n",
    "        end_index = min((i+1)*batch_size, nrof_images)\n",
    "        paths_batch = paths[start_index:end_index]\n",
    "        images = facenet.load_data(paths_batch, False, False, image_size)\n",
    "        feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n",
    "        emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)    \n",
    "        print(\"batch number:\", i)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    return emb_array\n",
    "\n",
    "def printDistance(emb_array, label_list, end=10):\n",
    "    threshold = 1.0\n",
    "    lng = len(emb_array[:end])\n",
    "    print('Distance matrix')\n",
    "    print('    ', end='')\n",
    "    for i in range(lng):\n",
    "        print('    %3d     ' % i, end='')\n",
    "    print('')\n",
    "    for i in range(lng):\n",
    "        print('%1d  ' % i, end='')\n",
    "        for j in range(lng):\n",
    "            dist = np.linalg.norm(emb_array[i,:] - emb_array[j,:])\n",
    "            print('  %1.4f' % dist, end='')\n",
    "            if (label_list[i] == label_list[j]) and (dist <= threshold):\n",
    "                print(\"(TP)\", end='')\n",
    "            elif (label_list[i] != label_list[j]) and (dist > threshold):\n",
    "                print(\"(TN)\", end='')\n",
    "            elif (label_list[i] == label_list[j]) and (dist > threshold):\n",
    "                print(\"\\033[93m(FN)\\033[0m\", end='')\n",
    "            elif (label_list[i] != label_list[j]) and (dist <= threshold):\n",
    "                print(\"\\033[93m(FP)\\033[0m\", end='')\n",
    "        print('')\n",
    "        \n",
    "def printData(labels, paths, number):\n",
    "    for i in range(number):\n",
    "        if i > 0 and labels[i] != labels[i-1]:\n",
    "            print('')\n",
    "        print('{0:3d} {1:3d}   {2:s}'.format(i, labels[i], paths[i].split(\"\\\\\")[1]))\n",
    "        # print('{0:3d}   {1:s}'.format(labels[i], paths[i].split(\"\\\\\")[1]))\n",
    "        \n",
    "def PCAtransform(emb_array, n_components = 10):\n",
    "    pca = PCA(n_components, whiten=True)\n",
    "    pca.fit(emb_array)\n",
    "    return pca.transform(emb_array), pca\n",
    "\n",
    "def splitTrainTest(data_set, percent_train):\n",
    "    paths, labels_list = facenet.get_image_paths_and_labels(data_set)\n",
    "    \n",
    "    lng = len(paths)\n",
    "    lng_train = int(lng * percent_train / 100);\n",
    "    lng_test = lng - lng_train\n",
    "    \n",
    "    train_paths = paths[0:lng_train]\n",
    "    test_paths = paths[lng_train:]\n",
    "    \n",
    "    train_labels = labels_list[0:lng_train]\n",
    "    test_labels = labels_list[lng_train:]\n",
    "    \n",
    "    return train_paths, train_labels, test_paths, test_labels\n",
    "\n",
    "def preprocessing(X, num_components = 50):\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    X = X - np.mean(X, axis=0) # centering\n",
    "    X = X / X.std(axis=0) # standardization\n",
    "\n",
    "    # PCA\n",
    "    cov = np.cov(X.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "\n",
    "    for ev in eig_vecs:\n",
    "        np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "\n",
    "    # Make a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "    # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "    eig_pairs.sort()\n",
    "    eig_pairs.reverse()\n",
    "\n",
    "    pca_matrix = eig_pairs[0][1].reshape(num_features, 1)\n",
    "\n",
    "    for i in range(1, num_components):\n",
    "        pca_matrix = np.hstack((pca_matrix,\n",
    "                                eig_pairs[i][1].reshape(num_features, 1)))\n",
    "\n",
    "    X = X.dot(pca_matrix)\n",
    "\n",
    "    # Whitening\n",
    "    X = X / np.sqrt(eig_vals[:num_components])\n",
    "    \n",
    "    return X\n",
    "    \n",
    "\n",
    "def trainFisher(emb_array, y):\n",
    "    emb_del = np.empty_like(emb_array)\n",
    "    np.copyto(emb_del, emb_array)\n",
    "    \n",
    "    emb_array = np.vstack((emb_array, y))\n",
    "    \n",
    "    E = np.linalg.inv(np.cov(emb_array.T))\n",
    "    D = (y - (1 / (len(emb_array) - 1) * np.sum(emb_del)))\n",
    "    \n",
    "    w = np.dot(E, D)\n",
    "    c = np.dot(w / np.linalg.norm(w), y)\n",
    "    \n",
    "    return c, w\n",
    "\n",
    "def trainFisherWithNum(emb_array, sample_number):    \n",
    "    y = emb_array[sample_number]\n",
    "    emb_del = np.delete(emb_array, (sample_number), axis=0)\n",
    "    \n",
    "    E = np.linalg.inv(np.cov(emb_array.T))\n",
    "    D = (y - (1 / (len(emb_array) - 1) * np.sum(emb_del)))\n",
    "    \n",
    "    w = np.dot(E, D)\n",
    "    c = np.dot(w / np.linalg.norm(w), y)\n",
    "    \n",
    "    return c, w   \n",
    "\n",
    "def Fisher(w, c, sample):\n",
    "    return np.dot(w / np.linalg.norm(w), sample) - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: 20170511-185253\n",
      "Metagraph file: model-20170511-185253.meta\n",
      "Checkpoint file: model-20170511-185253.ckpt-80000\n",
      "WARNING:tensorflow:The saved meta_graph is possibly from an older release:\n",
      "'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.\n",
      "INFO:tensorflow:Restoring parameters from 20170511-185253\\model-20170511-185253.ckpt-80000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "data_set = facenet.get_dataset(\"../../datasets/lfw/lfw_mtcnnpy_160/\")\n",
    "train_paths, train_labels, test_paths, test_labels = splitTrainTest(data_set, 100)\n",
    "load_model(\"20170511-185253\", sess)\n",
    "\n",
    "images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "image_size = 160\n",
    "embedding_size = embeddings.get_shape()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_array = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything ok!\n"
     ]
    }
   ],
   "source": [
    "t = emb_array\n",
    "\n",
    "# Centering\n",
    "tc = t - np.mean(t, axis=0)\n",
    "\n",
    "# Standartization\n",
    "ts = tc / tc.std(axis=0)\n",
    "\n",
    "# PCA\n",
    "X = ts\n",
    "\n",
    "num_components = 50\n",
    "\n",
    "cov = np.cov(X.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "\n",
    "for ev in eig_vecs:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "print('Everything is ok!')\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "pca_matrix = eig_pairs[0][1].reshape(128, 1)\n",
    "\n",
    "for i in range(1, num_components):\n",
    "    pca_matrix = np.hstack((pca_matrix,\n",
    "                            eig_pairs[i][1].reshape(128, 1)))\n",
    "\n",
    "X_pca = X.dot(pca_matrix)\n",
    "\n",
    "# Whitening\n",
    "X_whit = X_pca / np.sqrt(eig_vals[:num_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fisher's discriminant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall:  1.0\n"
     ]
    }
   ],
   "source": [
    "bush_label = 2\n",
    "idx, = np.where(train_labels == bush_label)\n",
    "\n",
    "X_work = np.delete(X_whit, idx, axis=0)\n",
    "c, w = trainFisher(X_work, X_whit[idx[0]])\n",
    "\n",
    "lng = len(X_whit)\n",
    "summ = 0\n",
    "thr = 0.7\n",
    "answers = []\n",
    "\n",
    "for i in range(lng):\n",
    "    l = Fisher(w, thr, X_whit[i]/np.linalg.norm(X_whit[i]))\n",
    "    answers.append(l)\n",
    "    \n",
    "answers = np.asarray(answers)\n",
    "true_labels = np.array(train_labels == bush_label, dtype=int)\n",
    "prediction = np.array(answers > 0, dtype=int)\n",
    "\n",
    "print(\"Recall: \", metrics.recall_score(true_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bush**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "\nArrays are not almost equal to 6 decimals\n\n(mismatch 100.0%)\n x: array(1.0)\n y: array(1.1433589280869492)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-205-65356c366671>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mX_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_whit\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mX_mask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocessing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_mask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mX_work\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_whit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-184-3eaa44c02465>\u001b[0m in \u001b[0;36mpreprocessing\u001b[1;34m(X, num_components)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mev\u001b[0m \u001b[1;32min\u001b[0m \u001b[0meig_vecs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtesting\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massert_array_almost_equal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;31m# Make a list of (eigenvalue, eigenvector) tuples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\testing\\utils.py\u001b[0m in \u001b[0;36massert_array_almost_equal\u001b[1;34m(x, y, decimal, err_msg, verbose)\u001b[0m\n\u001b[0;32m    960\u001b[0m     assert_array_compare(compare, x, y, err_msg=err_msg, verbose=verbose,\n\u001b[0;32m    961\u001b[0m              \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Arrays are not almost equal to %d decimals'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mdecimal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 962\u001b[1;33m              precision=decimal)\n\u001b[0m\u001b[0;32m    963\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\numpy\\testing\\utils.py\u001b[0m in \u001b[0;36massert_array_compare\u001b[1;34m(comparison, x, y, err_msg, verbose, header, precision, equal_nan, equal_inf)\u001b[0m\n\u001b[0;32m    776\u001b[0m                                 names=('x', 'y'), precision=precision)\n\u001b[0;32m    777\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 778\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    779\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mimport\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: \nArrays are not almost equal to 6 decimals\n\n(mismatch 100.0%)\n x: array(1.0)\n y: array(1.1433589280869492)"
     ]
    }
   ],
   "source": [
    "bush_label = 1871\n",
    "mask = (train_labels == bush_label)\n",
    "train_labels = np.asarray(train_labels)\n",
    "\n",
    "idx, = np.where(train_labels == bush_label)\n",
    "\n",
    "X_mask = X_whit[mask]\n",
    "X_mask = preprocessing(X_mask, 50)\n",
    "\n",
    "X_work = np.delete(X_whit, idx, axis=0)\n",
    "c, w = trainFisher(X_work, X_whit[idx[0]])\n",
    "\n",
    "lng = len(X_mask)\n",
    "summ = 0\n",
    "thr = 0.7\n",
    "answers = []\n",
    "\n",
    "for i in range(lng):\n",
    "    l = Fisher(w, thr, X_mask[i]/np.linalg.norm(X_mask[i]))\n",
    "    answers.append(l)\n",
    "\n",
    "answers = np.asarray(answers)\n",
    "true_labels = np.array(label_mask == bush_label, dtype=int)\n",
    "prediction = np.array(answers > 0, dtype=int)\n",
    "\n",
    "print('')\n",
    "print('True labels:    ', true_labels)\n",
    "print('Predicted labels', prediction)\n",
    "\n",
    "print(\"Recall: \", metrics.recall_score(true_labels, prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
