{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import math\n",
    "import re\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import facenet\n",
    "from scipy.spatial import distance\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN, MeanShift, estimate_bandwidth, Birch\n",
    "from sklearn import metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.cm as cm\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_filenames(model_dir):\n",
    "    files = os.listdir(model_dir)\n",
    "    meta_files = [s for s in files if s.endswith('.meta')]\n",
    "    if len(meta_files)==0:\n",
    "        raise ValueError('No meta file found in the model directory (%s)' % model_dir)\n",
    "    elif len(meta_files)>1:\n",
    "        raise ValueError('There should not be more than one meta file in the model directory (%s)' % model_dir)\n",
    "    meta_file = meta_files[0]\n",
    "    meta_files = [s for s in files if '.ckpt' in s]\n",
    "    max_step = -1\n",
    "    for f in files:\n",
    "        step_str = re.match(r'(^model-[\\w\\- ]+.ckpt-(\\d+))', f)\n",
    "        if step_str is not None and len(step_str.groups())>=2:\n",
    "            step = int(step_str.groups()[1])\n",
    "            if step > max_step:\n",
    "                max_step = step\n",
    "                ckpt_file = step_str.groups()[0]\n",
    "    return meta_file, ckpt_file\n",
    "\n",
    "def load_model(model, sess):\n",
    "    model_exp = os.path.expanduser(model)\n",
    "    print('Model directory: %s' % model_exp)\n",
    "    meta_file, ckpt_file = get_model_filenames(model_exp)\n",
    "\n",
    "    print('Metagraph file: %s' % meta_file)\n",
    "    print('Checkpoint file: %s' % ckpt_file)\n",
    "\n",
    "    saver = tf.train.import_meta_graph(os.path.join(model_exp, meta_file))\n",
    "    saver.restore(sess, os.path.join(model_exp, ckpt_file))\n",
    "    \n",
    "def forward(paths, batch_size):\n",
    "    nrof_images = len(paths)\n",
    "    nrof_batches = int(math.ceil(1.0*nrof_images / batch_size))\n",
    "    emb_array = np.zeros((nrof_images, embedding_size))\n",
    "\n",
    "    for i in range(nrof_batches):    \n",
    "        start_index = i*batch_size\n",
    "        end_index = min((i+1)*batch_size, nrof_images)\n",
    "        paths_batch = paths[start_index:end_index]\n",
    "        images = facenet.load_data(paths_batch, False, False, image_size)\n",
    "        feed_dict = { images_placeholder:images, phase_train_placeholder:False }\n",
    "        emb_array[start_index:end_index,:] = sess.run(embeddings, feed_dict=feed_dict)    \n",
    "        print(\"batch number:\", i)\n",
    "    print(\"Complete\")\n",
    "    \n",
    "    return emb_array\n",
    "\n",
    "def printDistance(emb_array, label_list, end=10):\n",
    "    threshold = 1.0\n",
    "    lng = len(emb_array[:end])\n",
    "    print('Distance matrix')\n",
    "    print('    ', end='')\n",
    "    for i in range(lng):\n",
    "        print('    %3d     ' % i, end='')\n",
    "    print('')\n",
    "    for i in range(lng):\n",
    "        print('%1d  ' % i, end='')\n",
    "        for j in range(lng):\n",
    "            dist = np.linalg.norm(emb_array[i,:] - emb_array[j,:])\n",
    "            print('  %1.4f' % dist, end='')\n",
    "            if (label_list[i] == label_list[j]) and (dist <= threshold):\n",
    "                print(\"(TP)\", end='')\n",
    "            elif (label_list[i] != label_list[j]) and (dist > threshold):\n",
    "                print(\"(TN)\", end='')\n",
    "            elif (label_list[i] == label_list[j]) and (dist > threshold):\n",
    "                print(\"\\033[93m(FN)\\033[0m\", end='')\n",
    "            elif (label_list[i] != label_list[j]) and (dist <= threshold):\n",
    "                print(\"\\033[93m(FP)\\033[0m\", end='')\n",
    "        print('')\n",
    "        \n",
    "def printData(labels, paths, number):\n",
    "    for i in range(number):\n",
    "        if i > 0 and labels[i] != labels[i-1]:\n",
    "            print('')\n",
    "        print('{0:3d} {1:3d}   {2:s}'.format(i, labels[i], paths[i].split(\"\\\\\")[1]))\n",
    "        # print('{0:3d}   {1:s}'.format(labels[i], paths[i].split(\"\\\\\")[1]))\n",
    "        \n",
    "def PCAtransform(emb_array, n_components = 10):\n",
    "    pca = PCA(n_components, whiten=True)\n",
    "    pca.fit(emb_array)\n",
    "    return pca.transform(emb_array), pca\n",
    "\n",
    "def splitTrainTest(data_set, percent_train):\n",
    "    paths, labels_list = facenet.get_image_paths_and_labels(data_set)\n",
    "    \n",
    "    lng = len(paths)\n",
    "    lng_train = int(lng * percent_train / 100);\n",
    "    lng_test = lng - lng_train\n",
    "    \n",
    "    train_paths = paths[0:lng_train]\n",
    "    test_paths = paths[lng_train:]\n",
    "    \n",
    "    train_labels = labels_list[0:lng_train]\n",
    "    test_labels = labels_list[lng_train:]\n",
    "    \n",
    "    return train_paths, train_labels, test_paths, test_labels\n",
    "\n",
    "def preprocessing(X, num_components = 50):\n",
    "    num_features = X.shape[1]\n",
    "    \n",
    "    X = X - np.mean(X, axis=0) # centering\n",
    "    X = X / X.std(axis=0) # standardization\n",
    "\n",
    "    # PCA\n",
    "    cov = np.cov(X.T)\n",
    "    eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "\n",
    "    for ev in eig_vecs:\n",
    "        np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "\n",
    "    # Make a list of (eigenvalue, eigenvector) tuples\n",
    "    eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "    # Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "    eig_pairs.sort()\n",
    "    eig_pairs.reverse()\n",
    "\n",
    "    pca_matrix = eig_pairs[0][1].reshape(num_features, 1)\n",
    "\n",
    "    for i in range(1, num_components):\n",
    "        pca_matrix = np.hstack((pca_matrix,\n",
    "                                eig_pairs[i][1].reshape(num_features, 1)))\n",
    "\n",
    "    X = X.dot(pca_matrix)\n",
    "\n",
    "    # Whitening\n",
    "    X = X / np.sqrt(eig_vals[:num_components])\n",
    "    \n",
    "    return X\n",
    "    \n",
    "\n",
    "def trainFisher(emb_array, y):\n",
    "    emb_del = np.empty_like(emb_array)\n",
    "    np.copyto(emb_del, emb_array)\n",
    "    \n",
    "    emb_array = np.vstack((emb_array, y))\n",
    "    \n",
    "    E = np.linalg.inv(np.cov(emb_array.T))\n",
    "    D = (y - (1 / (len(emb_array) - 1) * np.sum(emb_del)))\n",
    "    \n",
    "    w = np.dot(E, D)\n",
    "    c = np.dot(w / np.linalg.norm(w), y)\n",
    "    \n",
    "    return c, w\n",
    "\n",
    "def trainFisherWithNum(emb_array, sample_number):    \n",
    "    y = emb_array[sample_number]\n",
    "    emb_del = np.delete(emb_array, (sample_number), axis=0)\n",
    "    \n",
    "    E = np.linalg.inv(np.cov(emb_array.T))\n",
    "    D = (y - (1 / (len(emb_array) - 1) * np.sum(emb_del)))\n",
    "    \n",
    "    w = np.dot(E, D)\n",
    "    c = np.dot(w / np.linalg.norm(w), y)\n",
    "    \n",
    "    return c, w   \n",
    "\n",
    "def Fisher(w, c, sample):\n",
    "    return np.dot(w / np.linalg.norm(w), sample) - c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model directory: 20170511-185253\n",
      "Metagraph file: model-20170511-185253.meta\n",
      "Checkpoint file: model-20170511-185253.ckpt-80000\n",
      "WARNING:tensorflow:The saved meta_graph is possibly from an older release:\n",
      "'model_variables' collection should be of type 'byte_list', but instead is of type 'node_list'.\n",
      "INFO:tensorflow:Restoring parameters from 20170511-185253\\model-20170511-185253.ckpt-80000\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "data_set = facenet.get_dataset(\"../../datasets/lfw/lfw_mtcnnpy_160/\")\n",
    "train_paths, train_labels, test_paths, test_labels = splitTrainTest(data_set, 100)\n",
    "load_model(\"20170511-185253\", sess)\n",
    "\n",
    "images_placeholder = tf.get_default_graph().get_tensor_by_name(\"input:0\")\n",
    "embeddings = tf.get_default_graph().get_tensor_by_name(\"embeddings:0\")\n",
    "phase_train_placeholder = tf.get_default_graph().get_tensor_by_name(\"phase_train:0\")\n",
    "\n",
    "image_size = 160\n",
    "embedding_size = embeddings.get_shape()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb_array = np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything ok!\n"
     ]
    }
   ],
   "source": [
    "t = emb_array\n",
    "\n",
    "# Centering\n",
    "tc = t - np.mean(t, axis=0)\n",
    "\n",
    "# Standartization\n",
    "ts = tc / tc.std(axis=0)\n",
    "\n",
    "# PCA\n",
    "X = ts\n",
    "\n",
    "num_components = 50\n",
    "\n",
    "cov = np.cov(X.T)\n",
    "eig_vals, eig_vecs = np.linalg.eig(cov)\n",
    "\n",
    "for ev in eig_vecs:\n",
    "    np.testing.assert_array_almost_equal(1.0, np.linalg.norm(ev))\n",
    "print('Everything is ok!')\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[:,i]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eig_pairs.sort()\n",
    "eig_pairs.reverse()\n",
    "\n",
    "pca_matrix = eig_pairs[0][1].reshape(128, 1)\n",
    "\n",
    "for i in range(1, num_components):\n",
    "    pca_matrix = np.hstack((pca_matrix,\n",
    "                            eig_pairs[i][1].reshape(128, 1)))\n",
    "\n",
    "X_pca = X.dot(pca_matrix)\n",
    "\n",
    "# Whitening\n",
    "X_whit = X_pca / np.sqrt(eig_vals[:num_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fisher's discriminant**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision:  1.0\n",
      "Recall:     0.152830188679\n"
     ]
    }
   ],
   "source": [
    "bush_label = 1871\n",
    "idx, = np.where(train_labels == bush_label)\n",
    "\n",
    "X_work = np.delete(X_whit, idx, axis=0)\n",
    "c, w = trainFisher(X_work, X_whit[idx[0]])\n",
    "\n",
    "lng = len(X_whit)\n",
    "summ = 0\n",
    "thr = 0.7\n",
    "answers = []\n",
    "\n",
    "for i in range(lng):\n",
    "    l = Fisher(w, thr, X_whit[i]/np.linalg.norm(X_whit[i]))\n",
    "    answers.append(l)\n",
    "    \n",
    "answers = np.asarray(answers)\n",
    "true_labels = np.array(train_labels == bush_label, dtype=int)\n",
    "prediction = np.array(answers > 0, dtype=int)\n",
    "\n",
    "print(\"Precision: \", metrics.precision_score(true_labels, prediction))\n",
    "print(\"Recall:    \", metrics.recall_score(true_labels, prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**FN and FP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements:              530\n",
      "False Negatives:       449\n",
      "False Negatives Rate:  0.8471698113207548\n",
      "\n",
      "Elements:              12703\n",
      "False Positives:       0\n",
      "False Positives Rate:  0.0\n"
     ]
    }
   ],
   "source": [
    "fn = len(prediction[idx][prediction[idx] != true_labels[idx]])\n",
    "fnr = fn / len(true_labels[idx])\n",
    "\n",
    "print(\"Elements:             \", len(prediction[idx]))\n",
    "print(\"False Negatives:      \", fn)\n",
    "print(\"False Negatives Rate: \", fnr)\n",
    "print(\"\")\n",
    "\n",
    "idx2, = np.where(train_labels != bush_label)\n",
    "fp = len(prediction[idx2][prediction[idx2] != true_labels[idx2]])\n",
    "fpr = fp / len(true_labels[idx2])\n",
    "\n",
    "print(\"Elements:             \", len(prediction[idx2]))\n",
    "print(\"False Positives:      \", fp)\n",
    "print(\"False Positives Rate: \", fpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Array of FN and FP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 fnr:  0.0 fpr:  0.0\n",
      "1 fnr:  0.0 fpr:  0.0\n",
      "2 fnr:  0.0 fpr:  0.0\n",
      "3 fnr:  0.0 fpr:  0.0\n",
      "4 fnr:  0.0 fpr:  0.0\n",
      "5 fnr:  0.25 fpr:  0.0\n",
      "6 fnr:  0.0 fpr:  0.0\n",
      "7 fnr:  0.5 fpr:  0.0\n",
      "8 fnr:  0.0 fpr:  0.0\n",
      "9 fnr:  0.0 fpr:  0.0\n",
      "10 fnr:  0.0 fpr:  0.0\n",
      "11 fnr:  0.0 fpr:  0.0\n",
      "12 fnr:  0.0 fpr:  0.0\n",
      "13 fnr:  0.0 fpr:  0.0\n",
      "14 fnr:  0.75 fpr:  0.0\n",
      "15 fnr:  0.0 fpr:  0.0\n",
      "16 fnr:  0.0 fpr:  0.0\n",
      "17 fnr:  0.0 fpr:  0.0\n",
      "18 fnr:  0.0 fpr:  0.0\n",
      "19 fnr:  0.0 fpr:  0.0\n"
     ]
    }
   ],
   "source": [
    "labels = np.unique(train_labels)\n",
    "\n",
    "fps = []\n",
    "fns = []\n",
    "\n",
    "lng = len(X_whit)\n",
    "thr = 0.7\n",
    "\n",
    "for lb in labels[0:20]:\n",
    "    bush_label = lb\n",
    "    answers = []\n",
    "    idx, = np.where(train_labels == bush_label)\n",
    "\n",
    "    X_work = np.delete(X_whit, idx, axis=0)\n",
    "    c, w = trainFisher(X_work, X_whit[idx[0]])\n",
    "    \n",
    "    for i in range(lng):\n",
    "        l = Fisher(w, thr, X_whit[i]/np.linalg.norm(X_whit[i]))\n",
    "        answers.append(l)\n",
    "    \n",
    "    answers = np.asarray(answers)\n",
    "    true_labels = np.array(train_labels == bush_label, dtype=int)\n",
    "    prediction = np.array(answers > 0, dtype=int)\n",
    "    \n",
    "    fn = len(prediction[idx][prediction[idx] != true_labels[idx]])\n",
    "    fnr = fn / len(true_labels[idx])\n",
    "    \n",
    "    idx2, = np.where(train_labels != bush_label)\n",
    "    fp = len(prediction[idx2][prediction[idx2] != true_labels[idx2]])\n",
    "    fpr = fp / len(true_labels[idx2])\n",
    "    \n",
    "    fns.append(fnr)\n",
    "    fps.append(fpr)\n",
    "    \n",
    "    print(bush_label, \"fnr: \", fnr, \"fpr: \", fpr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
